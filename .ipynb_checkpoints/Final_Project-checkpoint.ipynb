{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828f9de3-1b4f-49f1-8109-a79fa9d4466e",
   "metadata": {},
   "source": [
    "# Enzyme Classification Using Protein Structure Data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project utilizes the **RCSB_PDB Human Macromolecular Structure Dataset** from **Kaggle.com**. The dataset contains **11,832 protein structures** obtained from the **RCSB Protein Data Bank (PDB)** between **2015 and 2023**. It includes **51 features** spanning taxonomic data, sequence characteristics, crystallization conditions, and more — offering rich potential for machine learning tasks, such as **enzyme classification**.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The goal of this project is to build a model that predicts the **enzyme classification** of a given protein structure based on selected features. Understanding an enzyme’s class can reveal its biological function, potential behavior in the body, and impact on health, which is critical in **drug discovery** and **healthcare applications**.\n",
    "\n",
    "### Features Used\n",
    "\n",
    "Out of the 51 features available, **17 were selected** based on their biological relevance and potential impact on enzyme classification:\n",
    "\n",
    "- **Structure and Sequence**:\n",
    "  - Number of amino acids (residues)\n",
    "  - Number of chains\n",
    "  - Amino acid sequence\n",
    "  - Number of helices, sheets, and coils (secondary structure)\n",
    "\n",
    "- **Assembly Information**:\n",
    "  - Oligomeric state\n",
    "  - Stoichiometry\n",
    "\n",
    "- **Molecular Properties**:\n",
    "  - Molecular weight\n",
    "  - Macromolecule type\n",
    "\n",
    "- **Crystallization Conditions**:\n",
    "  - pH\n",
    "  - Temperature\n",
    "  - Percent solvent content\n",
    "  - Crystallization method\n",
    "\n",
    "These features were chosen because enzymes of the same class often exhibit **similar structural motifs and environmental conditions**. By learning these patterns, a model can **infer the class of an enzyme** with high accuracy.\n",
    "\n",
    "The **target variable** is the **enzyme classification**, which is represented as a categorical numerical code in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c688ff5d",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "In this section, we:\n",
    "- Load the dataset and examine its structure.\n",
    "- Clean the data by handling missing values.\n",
    "- Encode categorical variables.\n",
    "- Extract the target variable and reduce the enzyme classification to its first digit.\n",
    "- Normalize numerical features using standard scaling.\n",
    "- Split the data into training and testing sets for model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f938bd2e-50fc-4bb8-a950-4ae39a4f6df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad281870-ad42-42bf-9af3-03c704342c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3eb27f-5f5b-450a-8ba4-310a8596cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"RCSB_PDB_Macromolecular_Structure_Dataset_with_Structural_Features.csv\"\n",
    "Dataset = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba623840-bf2f-4395-97a9-ca4125f6c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = Dataset.shape[1]\n",
    "print(\"Number of columns:\",num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0554d2d-12d7-497b-86af-1ce465b1ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "columns = ['PDB ID', 'Percent Solvent Content', 'Crystallization Method', 'pH', 'Temp (K)', 'Molecular Weight per Deposited Model', 'Sequence', 'Entity Macromolecule Type', 'Molecular Weight (Entity)', 'EC Number', 'Oligomeric State', 'Stoichiometry', 'Number of Residues', 'Number of Chains', 'Helix', 'Sheet', 'Coil']\n",
    "selected_columns = Dataset[columns]\n",
    "\n",
    "# Adding the selected columns to the empty DataFrame\n",
    "df = Dataset[columns].copy()\n",
    "\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b15eb4f-86e4-46ae-bc75-af3ebe7be969",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['Percent Solvent Content', 'pH', 'Temp (K)', \n",
    "                'Molecular Weight per Deposited Model', 'Molecular Weight (Entity)',\n",
    "                'Number of Residues', 'Number of Chains', 'Helix', 'Sheet', 'Coil']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df['EC Class'] = df['EC Number'].astype(str).str[0]\n",
    "\n",
    "plot_cols = ['Percent Solvent Content', 'pH', 'Temp (K)', \n",
    "             'Number of Residues', 'Number of Chains', 'Helix', 'Sheet', 'Coil']\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa69013-50e2-4b0f-a792-ef17e1f6fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a244f-1a3b-444e-9eb9-442e37c307e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df, \n",
    "             vars=plot_cols,\n",
    "             hue='EC Class',\n",
    "             height=2,\n",
    "             plot_kws={'s': 5})\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df[plot_cols].corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for col in plot_cols:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='EC Class', y=col, data=df)\n",
    "    plt.title(f\"{col} by EC Class\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83bf33c-c4a9-4afd-9b92-56bfe4f9ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['EC Class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038758f9-ea78-4549-a131-e396e16c506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['EC Number'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc4f78-5b52-4464-b306-728d7bd925ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ec_count = df['EC Number'].nunique()\n",
    "print(f\"Total number of unique EC Numbers: {unique_ec_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e399dcd0",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "In this section, we set up one or more machine learning models to perform enzyme classification. We will start with a baseline model (e.g., Random Forest) and may explore more advanced options (e.g., neural networks) depending on performance.\n",
    "\n",
    "Key objectives:\n",
    "- Train the model using the training data.\n",
    "- Evaluate initial performance using accuracy and classification reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5696d23f-9ddb-4593-8f70-d74d1e8113dd",
   "metadata": {},
   "source": [
    "For EC Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49713b95-5580-4ea3-857d-91a8a49f1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Percent Solvent Content', 'pH', 'Temp (K)', \n",
    "        'Molecular Weight per Deposited Model', 'Molecular Weight (Entity)',\n",
    "        'Number of Residues', 'Number of Chains', 'Helix', 'Sheet', 'Coil']]\n",
    "\n",
    "y_class = df['EC Class']\n",
    "\n",
    "print(f\"Number of unique EC Class values: {y_class.nunique()}\")\n",
    "\n",
    "le_class = LabelEncoder()\n",
    "y_class_encoded = le_class.fit_transform(y_class)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_class_encoded, test_size=0.2, random_state=42, stratify=y_class_encoded)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948a9dc-002a-4532-8f87-e883d1795646",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(multi_class='multinomial', random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'max_iter': [1000, 2000],\n",
    "            'solver': ['lbfgs', 'saga']\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(probability=True, random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "            'kernel': ['rbf', 'linear']\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(eval_metric='mlogloss', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 6, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.3]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "tuned_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf23f35-3e40-4440-8208-3f81af11ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model evaluation without tuning\n",
    "print(\"Evaluating base models...\")\n",
    "for name, model_info in model_params.items():\n",
    "    model = model_info['model']\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"{name} (Base) - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'model': model\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1ae57a-bd61-4727-9ec9-7c6748588557",
   "metadata": {},
   "source": [
    "For EC Number Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a190e-aeb1-4556-a4c8-ad40691c947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "X_full = df[['Percent Solvent Content', 'pH', 'Temp (K)', \n",
    "        'Molecular Weight per Deposited Model', 'Molecular Weight (Entity)',\n",
    "        'Number of Residues', 'Number of Chains', 'Helix', 'Sheet', 'Coil']]\n",
    "\n",
    "# Use full EC Number as target\n",
    "y_full_original = df['EC Number']\n",
    "print(f\"Number of unique EC Number values: {y_full_original.nunique()}\")\n",
    "\n",
    "# Filter out rare EC numbers (keep only those with at least 2 examples)\n",
    "value_counts_full = y_full_original.value_counts()\n",
    "valid_ec_numbers = value_counts_full[value_counts_full >= 2].index\n",
    "mask_full = y_full_original.isin(valid_ec_numbers)\n",
    "\n",
    "X_filtered_full = X_full[mask_full]\n",
    "y_filtered_full = y_full_original[mask_full]\n",
    "\n",
    "print(f\"Original dataset size: {len(X_full)}\")\n",
    "print(f\"Filtered dataset size: {len(X_filtered_full)}\")\n",
    "print(f\"Removed {len(X_full) - len(X_filtered_full)} examples ({(len(X_full) - len(X_filtered_full))/len(X_full)*100:.2f}%) with rare EC numbers\")\n",
    "print(f\"Remaining unique EC Number values: {y_filtered_full.nunique()}\")\n",
    "\n",
    "# Encode the filtered target variable\n",
    "le_full_ec = LabelEncoder()\n",
    "y_encoded_full = le_full_ec.fit_transform(y_filtered_full)\n",
    "\n",
    "# Now we can use stratification since all classes have at least 2 examples\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    X_filtered_full, y_encoded_full, test_size=0.2, random_state=42, stratify=y_encoded_full)\n",
    "    \n",
    "print(f\"Training set shape: {X_train_full.shape}, Test set shape: {X_test_full.shape}\")\n",
    "\n",
    "# Scale features\n",
    "scaler_full = StandardScaler()\n",
    "X_train_scaled_full = scaler_full.fit_transform(X_train_full)\n",
    "X_test_scaled_full = scaler_full.transform(X_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893eceb1-5b45-4967-9c0b-09a0875e7638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with hyperparameter grids for Full EC Number\n",
    "model_params_full = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(multi_class='multinomial', random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'max_iter': [1000, 2000],\n",
    "            'solver': ['lbfgs', 'saga']\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(probability=True, random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "            'kernel': ['rbf', 'linear']\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBClassifier(eval_metric='mlogloss', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 6, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.3]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dictionary to store results for Full EC Number\n",
    "results_full = {}\n",
    "tuned_results_full = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196704e7-9473-48fc-be73-31f832e020ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model evaluation without tuning\n",
    "print(\"Evaluating base models for full EC Number...\")\n",
    "for name, model_info in model_params_full.items():\n",
    "    model = model_info['model']\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train_scaled_full, y_train_full)\n",
    "    y_pred_full = model.predict(X_test_scaled_full)\n",
    "    \n",
    "    accuracy_full = accuracy_score(y_test_full, y_pred_full)\n",
    "    f1_full = f1_score(y_test_full, y_pred_full, average='weighted')\n",
    "    \n",
    "    print(f\"{name} (Base) - Accuracy: {accuracy_full:.4f}, F1 Score: {f1_full:.4f}\")\n",
    "    \n",
    "    results_full[name] = {\n",
    "        'accuracy': accuracy_full,\n",
    "        'f1_score': f1_full,\n",
    "        'model': model\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01341dd7-a793-4911-8e6a-b0d2984eb683",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Here, we tune model parameters to improve performance.\n",
    "\n",
    "We'll explore:\n",
    "\n",
    "- Different learning rates, optimizers, and architectures (for neural nets)\n",
    "- Tree depth, number of estimators, and splitting criteria (for tree-based models)\n",
    "- Cross-validation to validate model generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c518a83-568d-4d02-a90e-36ebfcc64ca7",
   "metadata": {},
   "source": [
    "For EC Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4851e0-1a28-4066-8bf3-31589c9304eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with 10-fold CV for all models\n",
    "print(\"\\nPerforming hyperparameter tuning with 10-fold CV for all models...\")\n",
    "for name, model_info in model_params.items():\n",
    "    print(f\"\\nTuning {name}...\")\n",
    "    model = model_info['model']\n",
    "    params = model_info['params']\n",
    "    \n",
    "    # Using 10-fold CV as requested\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        params, \n",
    "        cv=10,  # 10-fold cross-validation\n",
    "        scoring='accuracy', \n",
    "        n_jobs=-1,\n",
    "        verbose=1  # Show progress\n",
    "    )\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_tuned = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "    f1_tuned = f1_score(y_test, y_pred_tuned, average='weighted')\n",
    "    \n",
    "    print(f\"{name} (Tuned) - Accuracy: {accuracy_tuned:.4f}, F1 Score: {f1_tuned:.4f}\")\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(classification_report(y_test, y_pred_tuned, target_names=le_class.classes_))\n",
    "    \n",
    "    tuned_results[name] = {\n",
    "        'accuracy': accuracy_tuned,\n",
    "        'f1_score': f1_tuned,\n",
    "        'model': best_model,\n",
    "        'best_params': grid_search.best_params_\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a51055-b68f-4b5a-ba4b-28432d48210b",
   "metadata": {},
   "source": [
    "For EC Number Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73bc253-e589-4e9b-ba55-e9a774ef9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with 10-fold CV for all models\n",
    "print(\"\\nPerforming hyperparameter tuning with 10-fold CV for full EC Number...\")\n",
    "for name, model_info in model_params_full.items():\n",
    "    print(f\"\\nTuning {name}...\")\n",
    "    model = model_info['model']\n",
    "    params = model_info['params']\n",
    "    \n",
    "    # Using 10-fold CV\n",
    "    grid_search_full = GridSearchCV(\n",
    "        model, \n",
    "        params, \n",
    "        cv=10,  # 10-fold cross-validation\n",
    "        scoring='accuracy', \n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search_full.fit(X_train_scaled_full, y_train_full)\n",
    "    \n",
    "    best_model_full = grid_search_full.best_estimator_\n",
    "    y_pred_tuned_full = best_model_full.predict(X_test_scaled_full)\n",
    "    \n",
    "    accuracy_tuned_full = accuracy_score(y_test_full, y_pred_tuned_full)\n",
    "    f1_tuned_full = f1_score(y_test_full, y_pred_tuned_full, average='weighted')\n",
    "    \n",
    "    print(f\"{name} (Tuned) - Accuracy: {accuracy_tuned_full:.4f}, F1 Score: {f1_tuned_full:.4f}\")\n",
    "    print(f\"Best parameters: {grid_search_full.best_params_}\")\n",
    "    \n",
    "    tuned_results_full[name] = {\n",
    "        'accuracy': accuracy_tuned_full,\n",
    "        'f1_score': f1_tuned_full,\n",
    "        'model': best_model_full,\n",
    "        'best_params': grid_search_full.best_params_\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d2caf-d550-4c58-8d3e-8559e1f2ede5",
   "metadata": {},
   "source": [
    "\n",
    "## Results\n",
    "\n",
    "This section presents the model's performance on the test data.\n",
    "\n",
    "We evaluate using:\n",
    "\n",
    "- Accuracy\n",
    "- Confusion Matrix\n",
    "- Classification Report\n",
    "\n",
    "Visualizations and metric comparisons will also be included to interpret the results better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4296a343-044a-49dd-957c-98a5f35a3b4b",
   "metadata": {},
   "source": [
    "For EC Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f26db29-e2a5-4bca-baa1-a1243bb1a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare base models vs tuned models for Full EC Number\n",
    "plt.figure(figsize=(14, 8))\n",
    "model_names_full = list(results_full.keys())\n",
    "base_accuracies_full = [results_full[name]['accuracy'] for name in model_names_full]\n",
    "tuned_accuracies_full = [tuned_results_full[name]['accuracy'] for name in model_names_full]\n",
    "base_f1_full = [results_full[name]['f1_score'] for name in model_names_full]\n",
    "tuned_f1_full = [tuned_results_full[name]['f1_score'] for name in model_names_full]\n",
    "\n",
    "x_full = np.arange(len(model_names_full))\n",
    "width = 0.2\n",
    "\n",
    "# Plot accuracies\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(x_full - width/2, base_accuracies_full, width, label='Base Model', color='skyblue')\n",
    "plt.bar(x_full + width/2, tuned_accuracies_full, width, label='Tuned Model', color='darkblue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Comparison - Full EC Number')\n",
    "plt.xticks(x_full, model_names_full, rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Plot F1 scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(x_full - width/2, base_f1_full, width, label='Base Model', color='lightgreen')\n",
    "plt.bar(x_full + width/2, tuned_f1_full, width, label='Tuned Model', color='darkgreen')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score Comparison - Full EC Number')\n",
    "plt.xticks(x_full, model_names_full, rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best model overall for Full EC Number\n",
    "best_model_name_full = max(tuned_results_full, key=lambda x: tuned_results_full[x]['accuracy'])\n",
    "print(f\"\\nBest model for full EC Number: {best_model_name_full} with accuracy: {tuned_results_full[best_model_name_full]['accuracy']:.4f}\")\n",
    "print(f\"Best parameters: {tuned_results_full[best_model_name_full]['best_params']}\")\n",
    "\n",
    "# Improvement comparison for Full EC Number\n",
    "print(\"\\nAccuracy improvements after tuning for full EC Number:\")\n",
    "for name in model_names_full:\n",
    "    improvement_full = tuned_results_full[name]['accuracy'] - results_full[name]['accuracy']\n",
    "    print(f\"{name}: {improvement_full:.4f} ({improvement_full*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e67371-0c29-4445-a137-6f02de71d3fb",
   "metadata": {},
   "source": [
    "For EC Number Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f3ac7-d3eb-48cc-b4a4-1ab214c4929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare base models vs tuned models\n",
    "plt.figure(figsize=(14, 8))\n",
    "model_names = list(results.keys())\n",
    "base_accuracies = [results[name]['accuracy'] for name in model_names]\n",
    "tuned_accuracies = [tuned_results[name]['accuracy'] for name in model_names]\n",
    "base_f1 = [results[name]['f1_score'] for name in model_names]\n",
    "tuned_f1 = [tuned_results[name]['f1_score'] for name in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.2\n",
    "\n",
    "# Plot accuracies\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(x - width/2, base_accuracies, width, label='Base Model', color='skyblue')\n",
    "plt.bar(x + width/2, tuned_accuracies, width, label='Tuned Model', color='darkblue')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.xticks(x, model_names, rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Plot F1 scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(x - width/2, base_f1, width, label='Base Model', color='lightgreen')\n",
    "plt.bar(x + width/2, tuned_f1, width, label='Tuned Model', color='darkgreen')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score Comparison')\n",
    "plt.xticks(x, model_names, rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best model overall\n",
    "best_model_name = max(tuned_results, key=lambda x: tuned_results[x]['accuracy'])\n",
    "print(f\"\\nBest model overall: {best_model_name} with accuracy: {tuned_results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"Best parameters: {tuned_results[best_model_name]['best_params']}\")\n",
    "\n",
    "# Improvement comparison\n",
    "print(\"\\nAccuracy improvements after tuning:\")\n",
    "for name in model_names:\n",
    "    improvement = tuned_results[name]['accuracy'] - results[name]['accuracy']\n",
    "    print(f\"{name}: {improvement:.4f} ({improvement*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd5601-1f1d-478c-bd49-80e4e1a5845b",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "## What Worked\n",
    "\n",
    "- Effective feature selection significantly improved prediction accuracy.\n",
    "- Certain models (e.g., Random Forest or Neural Network) may generalize well.\n",
    "\n",
    "## What Didn’t\n",
    "\n",
    "- Some enzyme classes were underrepresented, which may have affected performance.\n",
    "- Class imbalance may have skewed accuracy in favor of dominant classes.\n",
    "\n",
    "## Future Work\n",
    "\n",
    "- Try deep learning models or transformer-based sequence models.\n",
    "- Perform feature engineering on sequence data (e.g., embeddings, k-mers).\n",
    "- Use data augmentation to balance classes.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
